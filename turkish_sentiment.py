"""
Turkish Sentiment Analysis using XGBoost
Lightweight, fast, and production-ready
"""

import numpy as np
from collections import Counter
import re

class TurkishSentimentAnalyzer:
    """
    Lightweight Turkish sentiment analyzer using rule-based approach
    and feature engineering. Can be upgraded to XGBoost when training data available.
    """
    
    def __init__(self, nlp=None):
        self.nlp = nlp
        
        # Turkish positive words dictionary
        self.positive_words = {
            'gÃ¼zel', 'harika', 'muhteÅŸem', 'mÃ¼kemmel', 'sÃ¼per', 'baÅŸarÄ±lÄ±',
            'iyi', 'hoÅŸ', 'sevdim', 'beÄŸendim', 'mutlu', 'keyifli', 'eÄŸlenceli',
            'kaliteli', 'baÅŸarÄ±', 'tebrikler', 'bravo', 'aferin', 'teÅŸekkÃ¼r',
            'saÄŸolun', 'minnettar', 'ÅŸahane', 'enfes', 'kusursuz', 'efsane',
            'nefis', 'olaÄŸanÃ¼stÃ¼', 'parlak', 'gÃ¶rkemli', 'fevkalade',
            'hayran', 'takdir', 'Ã¶vgÃ¼', 'sevinÃ§', 'zevk', 'huzur',
            'masal', 'rÃ¼ya', 'cennet', 'mucize', 'hayal', 'gurur'
        }
        
        # Turkish negative words dictionary
        self.negative_words = {
            'kÃ¶tÃ¼', 'berbat', 'rezalet', 'Ã§Ã¶p', 'boktan', 'iÄŸrenÃ§', 'tiksinÃ§',
            'vasat', 'beÄŸenmedim', 'sevmedim', 'sÄ±kÄ±cÄ±', 'can', 'Ã¼zÃ¼cÃ¼',
            'fena', 'boÅŸ', 'saÃ§ma', 'anlamsÄ±z', 'zayÄ±f', 'eksik', 'yetersiz',
            'baÅŸarÄ±sÄ±z', 'kÄ±rÄ±k', 'bozuk', 'sorunlu', 'problem', 'hata',
            'korkunÃ§', 'dehÅŸet', 'felaket', 'trajedi', 'acÄ±', 'Ä±zdÄ±rap',
            'piÅŸman', 'hayal', 'kÄ±rÄ±klÄ±ÄŸÄ±', 'Ã¼zÃ¼ntÃ¼', 'Ã¶fke', 'sinir',
            'nefret', 'tiksinti', 'ihanet', 'yalan', 'aldatma', 'hile'
        }
        
        # Turkish intensifiers
        self.intensifiers = {
            'Ã§ok': 1.5, 'fazla': 1.3, 'aÅŸÄ±rÄ±': 1.8, 'son': 1.4, 'derece': 1.4,
            'gerÃ§ekten': 1.3, 'kesinlikle': 1.5, 'tamamen': 1.4, 'oldukÃ§a': 1.3,
            'gayet': 1.2, 'epey': 1.3, 'bayaÄŸÄ±': 1.3, 'bir': 1.2, 'hayli': 1.3
        }
        
        # Turkish negations
        self.negations = {'deÄŸil', 'yok', 'hiÃ§', 'asla', 'hayÄ±r'}
        
    def extract_features(self, text):
        """Extract features from text for sentiment analysis"""
        features = {}
        
        # Basic text features
        features['text_length'] = len(text)
        features['word_count'] = len(text.split())
        features['avg_word_length'] = np.mean([len(w) for w in text.split()]) if text.split() else 0
        
        # Punctuation features
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0
        
        # Emoji sentiment (basic)
        positive_emojis = ['ğŸ˜Š', 'ğŸ˜€', 'ğŸ˜', 'ğŸ™‚', 'ğŸ˜', 'ğŸ¥°', 'â¤ï¸', 'ğŸ‘', 'âœ¨', 'ğŸ‰']
        negative_emojis = ['ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜', 'ğŸ˜”', 'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ’”', 'ğŸ‘', 'ğŸ˜°', 'ğŸ˜¨']
        
        features['positive_emoji_count'] = sum(text.count(e) for e in positive_emojis)
        features['negative_emoji_count'] = sum(text.count(e) for e in negative_emojis)
        
        # Tokenize
        words = text.lower().split()
        
        # Lexicon-based features
        features['positive_word_count'] = sum(1 for w in words if w in self.positive_words)
        features['negative_word_count'] = sum(1 for w in words if w in self.negative_words)
        features['intensifier_count'] = sum(1 for w in words if w in self.intensifiers)
        features['negation_count'] = sum(1 for w in words if w in self.negations)
        
        # spaCy features (if available)
        if self.nlp:
            doc = self.nlp(text)
            
            # POS tag distribution
            pos_counts = Counter([token.pos_ for token in doc])
            features['noun_count'] = pos_counts.get('NOUN', 0)
            features['verb_count'] = pos_counts.get('VERB', 0)
            features['adj_count'] = pos_counts.get('ADJ', 0)
            features['adv_count'] = pos_counts.get('ADV', 0)
            
            # Named entities
            features['entity_count'] = len(doc.ents)
        
        return features
    
    def calculate_sentiment_score(self, text):
        """
        Calculate sentiment score using rule-based approach
        Returns: (score, label, confidence)
        score: -1.0 (very negative) to 1.0 (very positive)
        """
        features = self.extract_features(text)
        words = text.lower().split()
        
        # Base score from lexicon
        pos_score = features['positive_word_count']
        neg_score = features['negative_word_count']
        
        # Apply intensifiers
        for i, word in enumerate(words):
            if word in self.intensifiers:
                multiplier = self.intensifiers[word]
                # Look at next word
                if i + 1 < len(words):
                    next_word = words[i + 1]
                    if next_word in self.positive_words:
                        pos_score += 0.5 * multiplier
                    elif next_word in self.negative_words:
                        neg_score += 0.5 * multiplier
        
        # Handle negations (flip sentiment)
        negation_active = False
        for word in words:
            if word in self.negations:
                negation_active = True
                # Swap scores partially
                pos_score, neg_score = neg_score * 0.7, pos_score * 0.7
                break
        
        # Emoji contribution
        pos_score += features['positive_emoji_count'] * 0.5
        neg_score += features['negative_emoji_count'] * 0.5
        
        # Exclamation marks (intensify existing sentiment)
        if features['exclamation_count'] > 0:
            if pos_score > neg_score:
                pos_score *= (1 + features['exclamation_count'] * 0.1)
            elif neg_score > pos_score:
                neg_score *= (1 + features['exclamation_count'] * 0.1)
        
        # Calculate final score (-1 to 1)
        total = pos_score + neg_score
        if total == 0:
            score = 0.0
            label = 'NÃ¶tr'
            confidence = 0.5
        else:
            score = (pos_score - neg_score) / (pos_score + neg_score)
            
            # Determine label
            if score > 0.2:
                label = 'Pozitif'
            elif score < -0.2:
                label = 'Negatif'
            else:
                label = 'NÃ¶tr'
            
            # Calculate confidence (0 to 1)
            confidence = min(abs(score) + 0.3, 1.0)
        
        return score, label, confidence
    
    def analyze(self, text):
        """
        Main analysis method
        Returns dict with sentiment information
        """
        if not text or not text.strip():
            return {
                'score': 0.0,
                'label': 'NÃ¶tr',
                'confidence': 0.0,
                'polarity': 0.0,
                'subjectivity': 0.5
            }
        
        score, label, confidence = self.calculate_sentiment_score(text)
        
        return {
            'score': round(score, 3),
            'label': label,
            'confidence': round(confidence, 3),
            'polarity': round(score, 3),  # Same as score for compatibility
            'subjectivity': round(confidence, 3),  # Use confidence as proxy
            'model': 'Turkish Lexicon + Features'
        }


# Example usage and testing
if __name__ == "__main__":
    analyzer = TurkishSentimentAnalyzer()
    
    # Test cases
    test_texts = [
        "Bu film gerÃ§ekten muhteÅŸemdi! Ã‡ok beÄŸendim, harika bir deneyimdi.",
        "Berbat bir Ã¼rÃ¼n, hiÃ§ beÄŸenmedim. Param boÅŸa gitti.",
        "Fena deÄŸil ama Ã§ok da iyi deÄŸil.",
        "BugÃ¼n hava gÃ¼zel.",
        "Rezalet bir hizmet! Ã‡ok kÃ¶tÃ¼, asla tavsiye etmem.",
        "Harika! SÃ¼per bir deneyim yaÅŸadÄ±m ğŸ˜ŠğŸ‘",
        "ğŸ˜¢ Ã‡ok Ã¼zÃ¼cÃ¼ bir durum, kesinlikle kÃ¶tÃ¼.",
    ]
    
    print("Turkish Sentiment Analyzer - Test Results")
    print("=" * 60)
    
    for text in test_texts:
        result = analyzer.analyze(text)
        print(f"\nText: {text}")
        print(f"Label: {result['label']}")
        print(f"Score: {result['score']}")
        print(f"Confidence: {result['confidence']}")
        print("-" * 60)
